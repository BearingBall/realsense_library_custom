{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f66f7294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import success, loaded device =  cuda:0\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import realsense_photo as rsp\n",
    "import CustomDataset as cdset\n",
    "import pickle\n",
    "import network_shell as nsh\n",
    "import os\n",
    "from IPython.display import display, clear_output\n",
    "from tqdm import *\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Import success, loaded device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5f540f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model load success\n"
     ]
    }
   ],
   "source": [
    "#loading architexture\n",
    "import segmentation_models_pytorch as smp\n",
    "learning_rate = 0.0005\n",
    "\n",
    "#creating new model\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=4,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=9,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "#or loading already trained model\n",
    "#model = torch.load(\"model\")\n",
    "\n",
    "criterion = smp.losses.JaccardLoss('multiclass', classes=None, log_loss=False, from_logits=True, smooth=0.0, eps=1e-07)\n",
    "#criterion = nn.NLLLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = optim.Adagrad(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.7)\n",
    "\n",
    "print(\"Model load success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a75d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader success, number of batchs:  10\n",
      "Val loader success, number of batchs:  3\n"
     ]
    }
   ],
   "source": [
    "#loading some series of photos\n",
    "train_loader, val_loader = nsh.makeLoaders([\"data/home\", \"data/old_corpus\", \"data/new_corpus\", \"data/big_pack4\"], batch_size = 10)\n",
    "print(\"Train loader success, number of batchs: \", len(train_loader))\n",
    "print(\"Val loader success, number of batchs: \", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ba4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym training status:\n",
      "Epoch [1/40]\n",
      "Current values: loss: 0.0000, train_acc: 0.0000, val_acc: 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress:  10%|██████▋                                                            | 1/10 [00:12<01:55, 12.87s/it]"
     ]
    }
   ],
   "source": [
    "#gym training\n",
    "num_epochs = 40\n",
    "\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "model = model.float()\n",
    "#model.to(device)\n",
    "\n",
    "ave_loss = 0\n",
    "ave_train_acc = 0\n",
    "ave_val_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    clear_output(wait=True)\n",
    "    print(\"Gym training status:\")\n",
    "    print(\"Epoch [{}/{}]\".format(epoch+1,num_epochs))\n",
    "    print(\"Current values: loss: {:.4f}, train_acc: {:.4f}, val_acc: {:.4f}\".format(ave_loss, ave_train_acc, ave_val_acc), flush=True)\n",
    "    nsh.resPlotter(loss_list, train_acc_list, val_acc_list)\n",
    "    \n",
    "    loss_accum = 0\n",
    "    train_acc_accum = 0\n",
    "    val_acc_accum = 0\n",
    "\n",
    "    train_bar = tqdm(total=len(train_loader), position = 0, leave = False, desc='Train progress')\n",
    "    for i, (images, labels, _) in enumerate(train_loader):\n",
    "        train_bar.update(1)\n",
    "        \n",
    "        # Прямой запуск\n",
    "        #images = images.to(device)\n",
    "        #labels = labels.to(device)\n",
    "        \n",
    "        model.train()\n",
    "        outputs = model(images.float())\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss_accum += loss\n",
    "\n",
    "        # Обратное распространение и оптимизатор\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Отслеживание точности\n",
    "        picture = nsh.preds_to_images(outputs)\n",
    "        picture = picture.reshape((picture.shape[0],1,picture.shape[1], picture.shape[2]))\n",
    "        \n",
    "        total = labels.size(0)*labels.size(2)*labels.size(3)\n",
    "        correct = (picture == labels).sum().item()\n",
    "        \n",
    "        train_acc_accum += correct / total\n",
    "        \n",
    "        ave_loss = loss_accum / (i+1)\n",
    "        ave_train_acc = train_acc_accum / (i+1)\n",
    "        \n",
    "        #print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, train acc: {:.4f}'\n",
    "        #      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), correct / total))\n",
    "    train_bar.close()\n",
    "    \n",
    "    \n",
    "    loss_list.append(ave_loss.detach().numpy())\n",
    "    train_acc_list.append(ave_train_acc)\n",
    "    \n",
    "    \n",
    "    if scheduler is not None:\n",
    "            scheduler.step(ave_loss)\n",
    "            \n",
    "    val_bar = tqdm(total=len(val_loader), position = 0, leave = False, desc='Val progress')        \n",
    "    for i, (images, labels,_) in enumerate(val_loader):\n",
    "        val_bar.update(1)\n",
    "        \n",
    "        outputs = model(images.float())\n",
    "        model.eval()\n",
    "        \n",
    "        picture = nsh.preds_to_images(outputs)\n",
    "        picture = picture.reshape((picture.shape[0],1,picture.shape[1], picture.shape[2]))\n",
    "        \n",
    "        total = labels.size(0)*labels.size(2)*labels.size(3)\n",
    "        correct = (picture == labels).sum().item()\n",
    "        val_acc_accum += correct / total\n",
    "        ave_val_acc = val_acc_accum / (i+1)\n",
    "        \n",
    "        #print('Epoch [{}/{}], Step [{}/{}], val acc: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, correct / total))\n",
    "    val_bar.close()\n",
    "    val_acc_list.append(ave_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd177cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  0  photo\n",
      "Val Loss: 0.2838, acc 0.9508\n",
      "loading  1  photo\n",
      "Val Loss: 0.3403, acc 0.9215\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-89ef287f6345>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#model.to(torch.device(\"cpu\"))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" photo\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m#print(images.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\степан\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\степан\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\степан\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\степан\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\depth_data\\segmentation_models.pytorch-master\\CustomDataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m640\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mpicture\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbg_removed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                 \u001b[0mpicture\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbg_removed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m                 \u001b[0mpicture\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbg_removed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0mpicture\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdepth_image\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model.to(torch.device(\"cpu\"))\n",
    "for i, (images, labels, _) in enumerate(val_loader):\n",
    "        print(\"loading \", i, \" photo\")\n",
    "        #print(images.shape)\n",
    "        outputs = model(images.float())\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        picture = nsh.preds_to_images(outputs)\n",
    "        total = labels.size(0)*labels.size(2)*labels.size(3)\n",
    "        correct = (picture == labels[:,0,:,:]).sum().item()\n",
    "\n",
    "        print('Val Loss: {:.4f}, acc {:.4f}'\n",
    "                  .format(loss.item(), correct/total))\n",
    "        \n",
    "        \n",
    "        \n",
    "        rgb_to_draw = np.transpose(images[:,0:3,:,:], (0,2,3,1))\n",
    "        label_pred_to_draw = np.transpose(picture[:,:,:], (0,1,2))\n",
    "        label_to_draw = np.transpose(labels[:,:,:,:], (1,0,2,3))\n",
    "        label_to_draw = label_to_draw[0]\n",
    "        \n",
    "        rsp.drawLabelMap(rgb_to_draw.numpy(), label_to_draw.numpy(), label_pred_to_draw.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39864f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"saves/\"+str(ave_val_acc))\n",
    "print(\"Model saved with values: loss: {:.4f}, train_acc: {:.4f}, val_acc: {:.4f}\".format(ave_loss, ave_train_acc, ave_val_acc), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef07381b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"saves/0.8668207465277779\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b284185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader success, number of batchs:  0\n",
      "Val loader success, number of batchs:  4\n"
     ]
    }
   ],
   "source": [
    "#loading some series of photos\n",
    "train_loader, val_loader = nsh.makeLoaders([\"data/test\"], \n",
    "                                           validation_fraction = 1, batch_size = 10)\n",
    "print(\"Train loader success, number of batchs: \", len(train_loader))\n",
    "print(\"Val loader success, number of batchs: \", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6be18b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
